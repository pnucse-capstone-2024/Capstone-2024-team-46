{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moter dataset Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect anomalies, we plan to input data into the model at 1-second intervals with a sampling rate of 100Hz. Therefore, we will segment the data into 70 samples for each 1-second intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_0 = pd.read_csv(\"data/label_0_full.csv\")\n",
    "label_1 = pd.read_csv(\"data/label_1_full.csv\")\n",
    "label_2 = pd.read_csv(\"data/label_2_full.csv\")\n",
    "label_3 = pd.read_csv(\"data/label_3_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445800\n",
      "1440100\n",
      "1414800\n",
      "1439900\n"
     ]
    }
   ],
   "source": [
    "len_0 = len(label_0)\n",
    "len_1 = len(label_1)\n",
    "len_2 = len(label_2)\n",
    "len_3 = len(label_3)\n",
    "\n",
    "label_X = [label_0, label_1, label_2, label_3]\n",
    "rem = [len_0, len_1, len_2, len_3]\n",
    "\n",
    "for i in range(4):\n",
    "    r = rem[i] % 100\n",
    "    label_X[i] = label_X[i].iloc[:rem[i] - r]\n",
    "\n",
    "    print(len(label_X[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14401 100\n"
     ]
    }
   ],
   "source": [
    "data_split_0 = np.array_split(label_X[0], len(label_X[0]) // 100)\n",
    "data_split_1 = np.array_split(label_X[1], len(label_X[1]) // 100)\n",
    "data_split_2 = np.array_split(label_X[2], len(label_X[2]) // 100)\n",
    "data_split_3 = np.array_split(label_X[3], len(label_X[3]) // 100)\n",
    "\n",
    "\n",
    "y_label_0 = np.zeros(len(data_split_0))\n",
    "y_label_1 = np.ones(len(data_split_1))\n",
    "y_label_2 = np.full(len(data_split_2), 2)\n",
    "y_label_3 = np.full(len(data_split_3), 3)\n",
    "\n",
    "print(len(data_split_1), len(data_split_1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, validataion, test set\n",
    "\n",
    "train_ratio = 0.7\n",
    "validation ratio = 0.15\n",
    "test ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599\n",
      "3598\n",
      "3599\n",
      "3599\n",
      "x_val size: 2160\n",
      "x_test size: 2160\n",
      "y_val size: 2160\n",
      "y_test size: 2160\n",
      "14395\n",
      "14395\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드 사용할 것.\n",
    "\n",
    "train_ratio = 0.7\n",
    "temp_ratio = 0.3\n",
    "val_ratio = 0.5\n",
    "test_ratio = temp_ratio / 2\n",
    "\n",
    "data_splits = [data_split_0, data_split_1, data_split_2, data_split_3]\n",
    "y_data_splits = [y_label_0, y_label_1, y_label_2, y_label_3]\n",
    "\n",
    "train_sizes = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "y_train_data = []\n",
    "y_test_data = []\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    data_len = len(data_splits[i])\n",
    "    train_size = int(data_len * train_ratio)\n",
    "    train_sizes.append(train_size)\n",
    "    train_data.append(data_splits[i][:train_size])\n",
    "    test_data.append(data_splits[i][train_size:])\n",
    "\n",
    "    y_train_data.append(y_data_splits[i][:train_size])\n",
    "    y_test_data.append(y_data_splits[i][train_size:])\n",
    "\n",
    "\n",
    "# check\n",
    "for i in range(4):\n",
    "    print(len(train_data[i]) + len(test_data[i]))\n",
    "\n",
    "x_train_concat = np.concatenate(train_data, axis = 0)\n",
    "x_test_concat = np.concatenate(test_data, axis = 0)\n",
    "y_train_concat = np.concatenate(y_train_data, axis=0)\n",
    "y_test_concat = np.concatenate(y_test_data, axis=0)\n",
    "\n",
    "\n",
    "val_size = int(x_test_concat.shape[0] * val_ratio)\n",
    "test_size = x_test_concat.shape[0] - val_size\n",
    "\n",
    "# validation & test set split\n",
    "x_val = x_test_concat[:val_size]\n",
    "x_test = x_test_concat[val_size:]\n",
    "\n",
    "y_val = y_test_concat[:val_size]\n",
    "y_test = y_test_concat[val_size:]\n",
    "\n",
    "# check\n",
    "print(f\"x_val size: {x_val.shape[0]}\")\n",
    "print(f\"x_test size: {x_test.shape[0]}\")\n",
    "print(f\"y_val size: {y_val.shape[0]}\")\n",
    "print(f\"y_test size: {y_test.shape[0]}\")\n",
    "\n",
    "# check \n",
    "print(x_train_concat.shape[0] + x_test.shape[0] + x_val.shape[0])\n",
    "print(y_train_concat.shape[0] + y_test.shape[0] + y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split 0: train + val + test = 10120 + 2169 + 2169\n",
      "Data split 1: train + val + test = 10080 + 2160 + 2161\n",
      "Data split 2: train + val + test = 9903 + 2122 + 2123\n",
      "Data split 3: train + val + test = 10079 + 2160 + 2160\n",
      "Train data shape: (40182, 100, 3)\n",
      "Validation data shape: (8611, 100, 3)\n",
      "Test data shape: (8613, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "temp_ratio = 0.3  # for both val and test combined\n",
    "val_ratio = 0.5  # 50% of temp_ratio for validation\n",
    "test_ratio = temp_ratio / 2  # same as val_ratio\n",
    "\n",
    "data_splits = [data_split_0, data_split_1, data_split_2, data_split_3]\n",
    "y_data_splits = [y_label_0, y_label_1, y_label_2, y_label_3]\n",
    "\n",
    "train_sizes = []\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "y_train_data = []\n",
    "y_val_data = []\n",
    "y_test_data = []\n",
    "\n",
    "for i in range(4):\n",
    "    data_len = len(data_splits[i])\n",
    "    train_size = int(data_len * train_ratio)\n",
    "    temp_size = data_len - train_size \n",
    "    val_size = int(temp_size * val_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_sizes.append(train_size)\n",
    "    train_data.append(data_splits[i][:train_size])\n",
    "    val_data.append(data_splits[i][train_size:train_size + val_size])\n",
    "    test_data.append(data_splits[i][train_size + val_size:])\n",
    "\n",
    "    # Split the labels\n",
    "    y_train_data.append(y_data_splits[i][:train_size])\n",
    "    y_val_data.append(y_data_splits[i][train_size:train_size + val_size])\n",
    "    y_test_data.append(y_data_splits[i][train_size + val_size:])\n",
    "\n",
    "# check the lengths of each split\n",
    "for i in range(4):\n",
    "    print(f\"Data split {i}: train + val + test =\", len(train_data[i]), \"+\", len(val_data[i]), \"+\", len(test_data[i]))\n",
    "\n",
    "# Concatenate all splits across the 4 data parts\n",
    "x_train_concat = np.concatenate(train_data, axis=0)\n",
    "x_val_concat = np.concatenate(val_data, axis=0)\n",
    "x_test_concat = np.concatenate(test_data, axis=0)\n",
    "\n",
    "y_train_concat = np.concatenate(y_train_data, axis=0)\n",
    "y_val_concat = np.concatenate(y_val_data, axis=0)\n",
    "y_test_concat = np.concatenate(y_test_data, axis=0)\n",
    "\n",
    "# Output final shapes to confirm correct splitting\n",
    "print(\"Train data shape:\", x_train_concat.shape)\n",
    "print(\"Validation data shape:\", x_val_concat.shape)\n",
    "print(\"Test data shape:\", x_test_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0.0: count = 2169, ratio = 25.19%\n",
      "Label 1.0: count = 2160, ratio = 25.08%\n",
      "Label 2.0: count = 2122, ratio = 24.64%\n",
      "Label 3.0: count = 2160, ratio = 25.08%\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each label in y_val_concat\n",
    "unique_labels, label_counts = np.unique(y_val_concat, return_counts=True)\n",
    "\n",
    "# Calculate the percentage of each label\n",
    "label_ratios = label_counts / len(y_val_concat)\n",
    "\n",
    "# Print the results\n",
    "for label, count, ratio in zip(unique_labels, label_counts, label_ratios):\n",
    "    print(f\"Label {label}: count = {count}, ratio = {ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40182, 100, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40182,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40182, 4)\n",
      "(8611, 4)\n",
      "(8613, 4)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert y_train and y_valid to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_concat, num_classes=4)  # Assuming classes are 0, 1, 2, 3\n",
    "y_valid_one_hot = to_categorical(y_val_concat, num_classes=4)  # Assuming you have a similar y_valid\n",
    "y_test_one_hot = to_categorical(y_test_concat, num_classes=4)  # Assuming you have a similar y_test\n",
    "\n",
    "# Check the shape\n",
    "print(y_train_one_hot.shape)  # Should be (10070, 4)\n",
    "print(y_valid_one_hot.shape) \n",
    "print(y_test_one_hot.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: count = 10120, ratio = 25.19%\n",
      "Class 1: count = 10080, ratio = 25.09%\n",
      "Class 2: count = 9903, ratio = 24.65%\n",
      "Class 3: count = 10079, ratio = 25.08%\n"
     ]
    }
   ],
   "source": [
    "# Sum along axis 0 to get the count of each class\n",
    "class_counts = np.sum(y_train_one_hot, axis=0)\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_ratios = class_counts / np.sum(class_counts)\n",
    "\n",
    "# Print the results\n",
    "for i, (count, ratio) in enumerate(zip(class_counts, class_ratios)):\n",
    "    print(f\"Class {i}: count = {int(count)}, ratio = {ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15294558232718816693\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11378904359985974365\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6933955910558672802\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15573965249812017374\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13699355476082648183\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(tf.__version__)\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import LSTM, Dropout, Dense, InputLayer\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7477 - loss: 0.5234 - val_accuracy: 0.8720 - val_loss: 0.3097\n",
      "Epoch 2/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9417 - loss: 0.1490 - val_accuracy: 0.8425 - val_loss: 0.5430\n",
      "Epoch 3/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9564 - loss: 0.1157 - val_accuracy: 0.8262 - val_loss: 0.7032\n",
      "Epoch 4/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9777 - loss: 0.0629 - val_accuracy: 0.8243 - val_loss: 1.5073\n",
      "Epoch 5/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9818 - loss: 0.0509 - val_accuracy: 0.8314 - val_loss: 0.9988\n",
      "Epoch 6/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0391 - val_accuracy: 0.9475 - val_loss: 0.1342\n",
      "Epoch 7/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9898 - loss: 0.0305 - val_accuracy: 0.9087 - val_loss: 0.2767\n",
      "Epoch 8/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9909 - loss: 0.0273 - val_accuracy: 0.9215 - val_loss: 0.2026\n",
      "Epoch 9/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9919 - loss: 0.0237 - val_accuracy: 0.9775 - val_loss: 0.0542\n",
      "Epoch 10/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9913 - loss: 0.0256 - val_accuracy: 0.9076 - val_loss: 0.2202\n",
      "Epoch 11/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9940 - loss: 0.0181 - val_accuracy: 0.9604 - val_loss: 0.1036\n",
      "Epoch 12/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0152 - val_accuracy: 0.9516 - val_loss: 0.1064\n",
      "Epoch 13/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0172 - val_accuracy: 0.9520 - val_loss: 0.1411\n",
      "Epoch 14/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0143 - val_accuracy: 0.9034 - val_loss: 0.3017\n",
      "Epoch 15/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0158 - val_accuracy: 0.8631 - val_loss: 0.2901\n",
      "Epoch 16/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9954 - loss: 0.0139 - val_accuracy: 0.9244 - val_loss: 0.1946\n",
      "Epoch 17/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0104 - val_accuracy: 0.9401 - val_loss: 0.1875\n",
      "Epoch 18/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 0.9120 - val_loss: 0.3162\n",
      "Epoch 19/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.9772 - val_loss: 0.0613\n",
      "Epoch 20/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9624 - val_loss: 0.1002\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # Conv Block 1 \n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\",\n",
    "                        input_shape=[100, 3]),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"), \n",
    "\n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Conv Block 2 \n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),           \n",
    "    \n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 3\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),   \n",
    "    \n",
    "    # Pooling layer        \n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 4\n",
    "    # keras.layers.Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # Updated output layer for multi-class classification\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")  # Change to 4 units and softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",  # Use categorical crossentropy\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])  # Accuracy metric is still appropriate\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_concat, y_train_one_hot,  # Ensure y_train is one-hot encoded for multi-class\n",
    "    epochs=20,  \n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_concat, y_valid_one_hot),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x_val_concat)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# # Save the model\n",
    "model.save(\"conv_net_v1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m98,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,822</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m683,822\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,940</span> (890.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m227,940\u001b[0m (890.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">455,882</span> (1.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m455,882\u001b[0m (1.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9200 - loss: 0.2053\n",
      "0.08895210176706314\n",
      "0.9662138819694519\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test_concat, y_test_one_hot, verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "[[9.0344763e-01 4.5467289e-07 8.1638232e-02 1.4913647e-02]\n",
      " [9.9052703e-01 8.9956170e-10 8.3286809e-03 1.1442734e-03]\n",
      " [9.6998024e-01 1.3512960e-10 2.9757194e-02 2.6258337e-04]\n",
      " [4.7275230e-01 1.5128370e-06 3.8859278e-07 5.2724582e-01]\n",
      " [9.6528345e-01 2.2283839e-09 3.3260286e-02 1.4562479e-03]\n",
      " [9.7205019e-01 1.8445256e-07 1.3541399e-02 1.4408243e-02]\n",
      " [9.9707592e-01 7.3112644e-10 1.6090515e-03 1.3150580e-03]\n",
      " [9.9900717e-01 4.0546053e-11 4.3790665e-04 5.5498938e-04]\n",
      " [9.9698806e-01 1.1271378e-11 2.8719525e-03 1.3995037e-04]\n",
      " [9.7297227e-01 2.3191459e-08 7.3355593e-05 2.6954386e-02]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_concat, verbose=1)\n",
    "\n",
    "print(y_pred[:10])\n",
    "print(y_test_concat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n",
      "혼동 행렬:\n",
      " [[  0   0   0   0]\n",
      " [  0   0   0   0]\n",
      " [540 540   0   0]\n",
      " [  0   0 540 540]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_pred의 형태 확인 및 정수형으로 변환\n",
    "if y_pred.ndim == 2:\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "elif y_pred.ndim == 1:\n",
    "    print(\"y_pred는 이미 클래스 인덱스입니다.\")\n",
    "\n",
    "# y_test 및 y_pred의 고유 클래스 추출\n",
    "unique_classes = np.unique(np.concatenate((y_test, y_pred)))  # 두 배열의 고유 클래스 합치기\n",
    "\n",
    "# 클래스 인덱스를 0부터 시작하도록 조정\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "y_test_mapped = np.array([class_to_index[cls] for cls in y_test])\n",
    "y_pred_mapped = np.array([class_to_index[cls] for cls in y_pred])\n",
    "\n",
    "\n",
    "print(len(y_test_mapped))\n",
    "print(len(y_pred_mapped))\n",
    "# 혼동 행렬 초기화\n",
    "num_classes = len(unique_classes)\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "for i in range(len(y_test)):\n",
    "    conf_matrix[y_test_mapped[i], y_pred_mapped[i]] += 1\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2160"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       540\n",
      "         1.0       1.00      1.00      1.00       540\n",
      "         2.0       1.00      1.00      1.00       540\n",
      "         3.0       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00      2160\n",
      "   macro avg       1.00      1.00      1.00      2160\n",
      "weighted avg       1.00      1.00      1.00      2160\n",
      "\n",
      "혼동 행렬:\n",
      "[[540   0   0   0]\n",
      " [  0 540   0   0]\n",
      " [  0   0 540   0]\n",
      " [  0   0   0 540]]\n"
     ]
    }
   ],
   "source": [
    "## v1 손대지 말것\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_concat, y_pred_argmax))\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test_concat, y_pred_argmax)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       540\n",
      "         1.0       1.00      1.00      1.00       540\n",
      "         2.0       1.00      1.00      1.00       540\n",
      "         3.0       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00      2160\n",
      "   macro avg       1.00      1.00      1.00      2160\n",
      "weighted avg       1.00      1.00      1.00      2160\n",
      "\n",
      "혼동 행렬:\n",
      "[[540   0   0   0]\n",
      " [  0 540   0   0]\n",
      " [  0   0 540   0]\n",
      " [  0   0   0 540]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## v2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_concat, y_pred_argmax))\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test_concat, y_pred_argmax)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.87      0.93      2169\n",
      "         1.0       1.00      1.00      1.00      2161\n",
      "         2.0       0.93      1.00      0.96      2123\n",
      "         3.0       0.95      0.99      0.97      2160\n",
      "\n",
      "    accuracy                           0.97      8613\n",
      "   macro avg       0.97      0.97      0.97      8613\n",
      "weighted avg       0.97      0.97      0.97      8613\n",
      "\n",
      "혼동 행렬:\n",
      "[[1894    0  154  121]\n",
      " [   0 2161    0    0]\n",
      " [   0    0 2123    0]\n",
      " [  16    0    0 2144]]\n"
     ]
    }
   ],
   "source": [
    "## v3 손대지 말것\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_concat, y_pred_argmax))\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test_concat, y_pred_argmax)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  11223329152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223332848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223390288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223384480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223399792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223398912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223454064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223456352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223460576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223462864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223464800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223463920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223685200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223689424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223679392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11223693824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1727790421.593994 7865154 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1727790421.594011 7865154 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-01 22:47:01.594132: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um\n",
      "2024-10-01 22:47:01.594739: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-01 22:47:01.594744: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um\n",
      "2024-10-01 22:47:01.601098: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-01 22:47:01.635471: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpb9jgw9um\n",
      "2024-10-01 22:47:01.646514: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 52381 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# .h5 모델 로드\n",
    "h5_model = tf.keras.models.load_model('conv_net_v1.h5')\n",
    "\n",
    "# .tflite 변환기 생성\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(h5_model)\n",
    "\n",
    "# 양자화 적용 (정수 양자화)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "\n",
    "# INT8 양자화를 위해 필요한 대표적인 데이터셋 함수 정의\n",
    "def representative_dataset():\n",
    "    for i in range(100):  # 임의로 100개의 샘플을 사용\n",
    "        # X_windows_balanced에서 일부 샘플을 선택\n",
    "        sample = x_train_concat[i]  # 각 sample의 shape은 (10, 3)\n",
    "        # shape을 (1, 10, 3)로 맞추기 위해 배치 차원 추가\n",
    "        yield [np.expand_dims(sample, axis=0).astype(np.float32)]\n",
    "\n",
    "# # INT8 양자화를 위해 필요한 대표적인 데이터셋 함수 정의\n",
    "# def representative_dataset():\n",
    "#     for _ in range(100):\n",
    "#         # (1, 10, 24)은 배치 크기 1, 10x24 크기의 입력 데이터를 의미합니다.\n",
    "#         yield [np.random.rand(1, 10, 24).astype(np.float32)]\n",
    "\n",
    "# # 대표 데이터셋 지정\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# 완전한 INT8 양자화를 위한 설정\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # INT8 연산만 허용\n",
    "converter.inference_input_type = tf.int8  # 입력 타입을 INT8로 설정\n",
    "converter.inference_output_type = tf.int8  # 출력 타입을 INT8로 설정\n",
    "\n",
    "\n",
    "# .tflite 모델로 변환\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# .tflite 모델 저장\n",
    "with open('conv_net_int8_v1_1.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 128) (None, 24, 128)\n",
      "(None, 96, 4)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 4), output.shape=(None, 96, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 72\u001b[0m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure y_train is one-hot encoded for multi-class\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid_one_hot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# class_weight=class_weights_dict,\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[early_stop]\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_val_concat)\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:554\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m     )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 4), output.shape=(None, 96, 4)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "def unet_model(input_shape):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder (Contracting Path)\n",
    "    c1 = keras.layers.Conv1D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    c1 = keras.layers.Conv1D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(c1)\n",
    "    p1 = keras.layers.MaxPooling1D(pool_size=2)(c1)\n",
    "\n",
    "    c2 = keras.layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(p1)\n",
    "    c2 = keras.layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(c2)\n",
    "    p2 = keras.layers.MaxPooling1D(pool_size=2)(c2)\n",
    "\n",
    "    c3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\")(p2)\n",
    "    c3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\")(c3)\n",
    "    p3 = keras.layers.MaxPooling1D(pool_size=2)(c3)\n",
    "\n",
    "    c4 = keras.layers.Conv1D(256, kernel_size=3, padding=\"same\", activation=\"relu\")(p3)\n",
    "    c4 = keras.layers.Conv1D(256, kernel_size=3, padding=\"same\", activation=\"relu\")(c4)\n",
    "    p4 = keras.layers.MaxPooling1D(pool_size=2)(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = keras.layers.Conv1D(512, kernel_size=3, padding=\"same\", activation=\"relu\")(p4)\n",
    "    bottleneck = keras.layers.Conv1D(512, kernel_size=3, padding=\"same\", activation=\"relu\")(bottleneck)\n",
    "\n",
    "    # Decoder (Expanding Path)\n",
    "    u4 = keras.layers.Conv1DTranspose(256, kernel_size=2, strides=2, padding=\"same\")(bottleneck)\n",
    "    u4 = keras.layers.concatenate([u4, c4])\n",
    "    c5 = keras.layers.Conv1D(256, kernel_size=3, padding=\"same\", activation=\"relu\")(u4)\n",
    "    c5 = keras.layers.Conv1D(256, kernel_size=3, padding=\"same\", activation=\"relu\")(c5)\n",
    "\n",
    "    u3 = keras.layers.Conv1DTranspose(128, kernel_size=2, strides=2, padding=\"same\")(c5)\n",
    "    # c3_adjusted = keras.layers.Conv1D(128, kernel_size=1, padding=\"same\")(c3) \n",
    "    c3_adjusted = keras.layers.Cropping1D(cropping=(1, 0))(c3)  # 마지막 1개 타임스텝 제거\n",
    "\n",
    "    print(c3_adjusted.shape, u3.shape)\n",
    "    u3 = keras.layers.concatenate([u3, c3_adjusted])\n",
    "    c6 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\")(u3)\n",
    "    c6 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\")(c6)\n",
    "\n",
    "    u2 = keras.layers.Conv1DTranspose(64, kernel_size=2, strides=2, padding=\"same\")(c6)\n",
    "    c2_adjusted = keras.layers.Cropping1D(cropping=(2, 0))(c2)  # 마지막 1개 타임스텝 제거\n",
    "    u2 = keras.layers.concatenate([u2, c2_adjusted])\n",
    "    c7 = keras.layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(u2)\n",
    "    c7 = keras.layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(c7)\n",
    "\n",
    "    u1 = keras.layers.Conv1DTranspose(32, kernel_size=2, strides=2, padding=\"same\")(c7)\n",
    "    c1_adjusted = keras.layers.Cropping1D(cropping=(4, 0))(c1)  # 마지막 1개 타임스텝 제거\n",
    "    u1 = keras.layers.concatenate([u1, c1_adjusted])\n",
    "    c8 = keras.layers.Conv1D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(u1)\n",
    "    c8 = keras.layers.Conv1D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(c8)\n",
    "\n",
    "    outputs = keras.layers.Conv1D(4, kernel_size=1, activation=\"softmax\")(c8)  # Change to 4 for multi-class output\n",
    "    print(outputs.shape)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = (100, 3)  # Modify based on your actual input shape\n",
    "model = unet_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_concat, y_train_one_hot,  # Ensure y_train is one-hot encoded for multi-class\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_concat, y_valid_one_hot),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x_val_concat)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# Save the model\n",
    "model.save(\"u_net_v1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.6625 - loss: 0.2973 - val_accuracy: 0.9133 - val_loss: 0.1051\n",
      "Epoch 2/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9276 - loss: 0.0885 - val_accuracy: 0.7845 - val_loss: 0.5271\n",
      "Epoch 3/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9497 - loss: 0.0623 - val_accuracy: 0.6471 - val_loss: 2.0219\n",
      "Epoch 4/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9685 - loss: 0.0434 - val_accuracy: 0.6367 - val_loss: 7.0846\n",
      "Epoch 5/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9754 - loss: 0.0342 - val_accuracy: 0.6574 - val_loss: 4.1364\n",
      "Epoch 6/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0220 - val_accuracy: 0.6193 - val_loss: 0.8876\n",
      "Epoch 7/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9843 - loss: 0.0223 - val_accuracy: 0.8033 - val_loss: 0.5506\n",
      "Epoch 8/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0170 - val_accuracy: 0.7724 - val_loss: 0.7566\n",
      "Epoch 9/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9892 - loss: 0.0154 - val_accuracy: 0.8272 - val_loss: 0.6190\n",
      "Epoch 10/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9927 - loss: 0.0120 - val_accuracy: 0.8272 - val_loss: 0.4170\n",
      "Epoch 11/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9905 - loss: 0.0151 - val_accuracy: 0.8365 - val_loss: 0.2458\n",
      "Epoch 12/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9926 - loss: 0.0118 - val_accuracy: 0.8750 - val_loss: 0.2235\n",
      "Epoch 13/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9916 - loss: 0.0134 - val_accuracy: 0.9686 - val_loss: 0.0387\n",
      "Epoch 14/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0093 - val_accuracy: 0.9734 - val_loss: 0.0319\n",
      "Epoch 15/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9934 - loss: 0.0098 - val_accuracy: 0.8495 - val_loss: 0.2888\n",
      "Epoch 16/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0084 - val_accuracy: 0.8318 - val_loss: 0.4344\n",
      "Epoch 17/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0075 - val_accuracy: 0.8702 - val_loss: 0.1949\n",
      "Epoch 18/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9938 - loss: 0.0095 - val_accuracy: 0.9000 - val_loss: 0.1491\n",
      "Epoch 19/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0077 - val_accuracy: 0.8440 - val_loss: 0.4015\n",
      "Epoch 20/20\n",
      "\u001b[1m1256/1256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0061 - val_accuracy: 0.8351 - val_loss: 0.6267\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # Conv Block 1 \n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\",\n",
    "                        input_shape=[100, 3]),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"), \n",
    "\n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Conv Block 2 \n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),           \n",
    "    \n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 3\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),   \n",
    "    \n",
    "    # Pooling layer        \n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 4\n",
    "    # keras.layers.Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # Updated output layer for multi-class classification\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation=\"sigmoid\")  # Change to 4 units and softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\",  # Use categorical crossentropy\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])  # Accuracy metric is still appropriate\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_concat, y_train_one_hot,  # Ensure y_train is one-hot encoded for multi-class\n",
    "    epochs=20,  \n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_concat, y_valid_one_hot),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x_val_concat)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# # Save the model\n",
    "model.save(\"conv_net_v2.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99945641e-01, 4.95489029e-17, 2.61610154e-07, 1.38890509e-05],\n",
       "       [9.99852240e-01, 9.63207010e-16, 4.83867495e-07, 5.63673239e-05],\n",
       "       [9.92969811e-01, 6.21100463e-11, 5.78368963e-06, 5.58351120e-03],\n",
       "       ...,\n",
       "       [1.16577063e-07, 1.35192211e-04, 7.40288973e-20, 9.99954045e-01],\n",
       "       [1.05082984e-06, 1.21905752e-04, 2.65338427e-18, 9.99901235e-01],\n",
       "       [6.39405195e-10, 1.05226005e-04, 1.20326106e-23, 9.99913812e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4077 - loss: 2.3237\n",
      "0.9807576537132263\n",
      "0.7492163181304932\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test_concat, y_test_one_hot, verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8613, 34452]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 실제 레이블(y_true)도 이진 형식이어야 합니다.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 예: y_true = [[0, 1, 0, 0], [1, 0, 1, 0], ...]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 혼동 행렬 계산\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 실제 레이블과 예측된 레이블을 flatten하여 사용\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_concat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 혼동 행렬 시각화\u001b[39;00m\n\u001b[1;32m     19\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm)\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/sklearn/metrics/_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    248\u001b[0m     {\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8613, 34452]"
     ]
    }
   ],
   "source": [
    "## v4 손대지 말것 sigmoid\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "# 예측값 얻기\n",
    "y_pred = model.predict(x_test_concat)\n",
    "\n",
    "# 특정 임계값(예: 0.5)을 기준으로 레이블 변환\n",
    "threshold = 0.5\n",
    "y_pred_labels = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# 실제 레이블(y_true)도 이진 형식이어야 합니다.\n",
    "# 예: y_true = [[0, 1, 0, 0], [1, 0, 1, 0], ...]\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "# 실제 레이블과 예측된 레이블을 flatten하여 사용\n",
    "cm = confusion_matrix(y_test_concat.flatten(), y_pred_labels.flatten())\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_27\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_27\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">112,480</span> (439.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m112,480\u001b[0m (439.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">112,480</span> (439.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m112,480\u001b[0m (439.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  11171954384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172433584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172441680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172444144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172467936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172470928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172473568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172475856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172478320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172679168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172681104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172683392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172684800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  11172687440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1727790329.394891 7865154 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1727790329.395834 7865154 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-01 22:45:29.397741: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg\n",
      "2024-10-01 22:45:29.398583: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-01 22:45:29.398589: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg\n",
      "2024-10-01 22:45:29.407616: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-01 22:45:29.493965: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpwxp4mwsg\n",
      "2024-10-01 22:45:29.505333: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 107592 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "# softmax without\n",
    "# 기존 모델 로드\n",
    "model = keras.models.load_model(\"conv_net_v1.h5\")\n",
    "\n",
    "# 모델 호출로 입력을 정의합니다.\n",
    "example_input = np.random.rand(1, 100, 3)  # 예제 입력 정의\n",
    "_ = model(example_input)  # 모델 호출하여 입력을 정의합니다.\n",
    "\n",
    "# 소프트맥스 레이어 제외한 모델 생성\n",
    "model_without_softmax = keras.models.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# 확인\n",
    "model_without_softmax.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_without_softmax)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "\n",
    "\n",
    "\n",
    "# INT8 양자화를 위해 필요한 대표적인 데이터셋 함수 정의\n",
    "def representative_dataset():\n",
    "    for i in range(100):  # 임의로 100개의 샘플을 사용\n",
    "        # X_windows_balanced에서 일부 샘플을 선택\n",
    "        sample = x_train_concat[i]  # 각 sample의 shape은 (10, 3)\n",
    "        # shape을 (1, 10, 3)로 맞추기 위해 배치 차원 추가\n",
    "        yield [np.expand_dims(sample, axis=0).astype(np.float32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# 완전한 INT8 양자화를 위한 설정\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # INT8 연산만 허용\n",
    "converter.inference_input_type = tf.int8  # 입력 타입을 INT8로 설정\n",
    "converter.inference_output_type = tf.int8  # 출력 타입을 INT8로 설정\n",
    "\n",
    "\n",
    "# .tflite 모델로 변환\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "# 6. TFLite 모델 저장\n",
    "with open('conv_net_model_without_softmax.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
