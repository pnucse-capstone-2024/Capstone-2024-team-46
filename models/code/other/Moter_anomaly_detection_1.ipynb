{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moter dataset Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'data/sensor_data_b.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mdataset\u001b[49m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m      4\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_counts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(dataset, header=None) \n",
    "label_counts = df.iloc[:, -1].value_counts()\n",
    "\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m label_0_data \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m label_1_data \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m label_2_data \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "label_0_data = df[df.iloc[:, -1] == 0]\n",
    "label_1_data = df[df.iloc[:, -1] == 1]\n",
    "label_2_data = df[df.iloc[:, -1] == 2]\n",
    "\n",
    "label_0_data.to_csv('data/label_0_data.csv', index=False)\n",
    "label_1_data.to_csv('data/label_1_data.csv', index=False)\n",
    "label_1_data.to_csv('data/label_2_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect anomalies, we plan to input data into the model at 1-second intervals with a sampling rate of 100Hz. Therefore, we will segment the data into 70 samples for each 1-second intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_0 = pd.read_csv(\"data/label_0_data.csv\")\n",
    "label_1 = pd.read_csv(\"data/label_1b_data.csv\")\n",
    "label_2 = pd.read_csv(\"data/label_2_data.csv\")\n",
    "label_3 = pd.read_csv(\"data/label_3_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359900\n",
      "359800\n",
      "359900\n",
      "359900\n"
     ]
    }
   ],
   "source": [
    "len_0 = len(label_0)\n",
    "len_1 = len(label_1)\n",
    "len_2 = len(label_2)\n",
    "len_3 = len(label_3)\n",
    "\n",
    "label_X = [label_0, label_1, label_2, label_3]\n",
    "rem = [len_0, len_1, len_2, len_3]\n",
    "\n",
    "for i in range(4):\n",
    "    r = rem[i] % 100\n",
    "    label_X[i] = label_X[i].iloc[:rem[i] - r]\n",
    "\n",
    "    print(len(label_X[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_0 = np.array_split(label_X[0], len(label_X[0]) // 100)\n",
    "data_split_1 = np.array_split(label_X[1], len(label_X[1]) // 100)\n",
    "data_split_2 = np.array_split(label_X[2], len(label_X[2]) // 100)\n",
    "data_split_3 = np.array_split(label_X[3], len(label_X[3]) // 100)\n",
    "\n",
    "\n",
    "y_label_0 = np.zeros(len(data_split_0))\n",
    "y_label_1 = np.ones(len(data_split_1))\n",
    "y_label_2 = np.full(len(data_split_2), 2)\n",
    "y_label_3 = np.full(len(data_split_3), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, validataion, test set\n",
    "\n",
    "train_ratio = 0.7\n",
    "validation ratio = 0.15\n",
    "test ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599\n",
      "3598\n",
      "3599\n",
      "3599\n",
      "x_val size: 2160\n",
      "x_test size: 2160\n",
      "y_val size: 2160\n",
      "y_test size: 2160\n",
      "14395\n",
      "14395\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "temp_ratio = 0.3\n",
    "val_ratio = 0.5\n",
    "test_ratio = temp_ratio / 2\n",
    "\n",
    "data_splits = [data_split_0, data_split_1, data_split_2, data_split_3]\n",
    "y_data_splits = [y_label_0, y_label_1, y_label_2, y_label_3]\n",
    "\n",
    "train_sizes = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "y_train_data = []\n",
    "y_test_data = []\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    data_len = len(data_splits[i])\n",
    "    train_size = int(data_len * train_ratio)\n",
    "    train_sizes.append(train_size)\n",
    "    train_data.append(data_splits[i][:train_size])\n",
    "    test_data.append(data_splits[i][train_size:])\n",
    "\n",
    "    y_train_data.append(y_data_splits[i][:train_size])\n",
    "    y_test_data.append(y_data_splits[i][train_size:])\n",
    "\n",
    "\n",
    "# check\n",
    "for i in range(4):\n",
    "    print(len(train_data[i]) + len(test_data[i]))\n",
    "\n",
    "x_train_concat = np.concatenate(train_data, axis = 0)\n",
    "x_test_concat = np.concatenate(test_data, axis = 0)\n",
    "y_train_concat = np.concatenate(y_train_data, axis=0)\n",
    "y_test_concat = np.concatenate(y_test_data, axis=0)\n",
    "\n",
    "\n",
    "val_size = int(x_test_concat.shape[0] * val_ratio)\n",
    "test_size = x_test_concat.shape[0] - val_size\n",
    "\n",
    "# validation & test set split\n",
    "x_val = x_test_concat[:val_size]\n",
    "x_test = x_test_concat[val_size:]\n",
    "\n",
    "y_val = y_test_concat[:val_size]\n",
    "y_test = y_test_concat[val_size:]\n",
    "\n",
    "# check\n",
    "print(f\"x_val size: {x_val.shape[0]}\")\n",
    "print(f\"x_test size: {x_test.shape[0]}\")\n",
    "print(f\"y_val size: {y_val.shape[0]}\")\n",
    "print(f\"y_test size: {y_test.shape[0]}\")\n",
    "\n",
    "# check \n",
    "print(x_train_concat.shape[0] + x_test.shape[0] + x_val.shape[0])\n",
    "print(y_train_concat.shape[0] + y_test.shape[0] + y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10075, 100, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10075,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10075, 4)\n",
      "(2160, 4)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert y_train and y_valid to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_concat, num_classes=4)  # Assuming classes are 0, 1, 2, 3\n",
    "y_valid_one_hot = to_categorical(y_val, num_classes=4)  # Assuming you have a similar y_valid\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=4)  # Assuming you have a similar y_test\n",
    "\n",
    "# Check the shape\n",
    "print(y_train_one_hot.shape)  # Should be (10070, 4)\n",
    "print(y_valid_one_hot.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15294558232718816693\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11378904359985974365\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6933955910558672802\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15573965249812017374\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13699355476082648183\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(tf.__version__)\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import LSTM, Dropout, Dense, InputLayer\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10075 samples, validate on 2160 samples\n",
      "Epoch 1/20\n",
      "10075/10075 [==============================] - 7s 714us/step - loss: 0.4110 - accuracy: 0.8461 - val_loss: 0.0023 - val_accuracy: 0.9995\n",
      "Epoch 2/20\n",
      "10075/10075 [==============================] - 7s 711us/step - loss: 2.5682e-04 - accuracy: 0.9999 - val_loss: 0.0015 - val_accuracy: 0.9995\n",
      "Epoch 3/20\n",
      "10075/10075 [==============================] - 7s 709us/step - loss: 5.3878e-06 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 4/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 2.1092e-06 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 5/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 9.8702e-07 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
      "Epoch 6/20\n",
      "10075/10075 [==============================] - 7s 707us/step - loss: 5.4053e-07 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 7/20\n",
      "10075/10075 [==============================] - 7s 702us/step - loss: 3.2938e-07 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 8/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 2.4309e-07 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 0.9995\n",
      "Epoch 9/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 1.7027e-07 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 10/20\n",
      "10075/10075 [==============================] - 7s 709us/step - loss: 1.0606e-07 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 0.9995\n",
      "Epoch 11/20\n",
      "10075/10075 [==============================] - 7s 709us/step - loss: 9.1793e-08 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 12/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 5.4120e-08 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 0.9995\n",
      "Epoch 13/20\n",
      "10075/10075 [==============================] - 7s 714us/step - loss: 5.5362e-08 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 14/20\n",
      "10075/10075 [==============================] - 7s 706us/step - loss: 3.4526e-08 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 15/20\n",
      "10075/10075 [==============================] - 7s 709us/step - loss: 2.6433e-08 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "10075/10075 [==============================] - 7s 714us/step - loss: 2.0754e-08 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "10075/10075 [==============================] - 7s 704us/step - loss: 1.7322e-08 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 18/20\n",
      "10075/10075 [==============================] - 7s 706us/step - loss: 1.3867e-08 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 19/20\n",
      "10075/10075 [==============================] - 7s 708us/step - loss: 1.2400e-08 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 20/20\n",
      "10075/10075 [==============================] - 7s 700us/step - loss: 1.0353e-08 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # Conv Block 1 \n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\",\n",
    "                        input_shape=[100, 3]),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"), \n",
    "\n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Conv Block 2 \n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),           \n",
    "    \n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 3\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),   \n",
    "    \n",
    "    # Pooling layer        \n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 4\n",
    "    keras.layers.Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # Updated output layer for multi-class classification\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")  # Change to 4 units and softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",  # Use categorical crossentropy\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])  # Accuracy metric is still appropriate\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_concat, y_train_one_hot,  # Ensure y_train is one-hot encoded for multi-class\n",
    "    epochs=20,  \n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_valid_one_hot),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"u_net3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 0s 195us/step\n",
      "1.4681439769283468e-05\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test_one_hot, verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 0s 186us/step\n",
      "[[5.1180602e-09 8.2138288e-16 1.0000000e+00 3.1631824e-08]\n",
      " [4.1516701e-09 9.1624189e-16 1.0000000e+00 3.9026578e-08]\n",
      " [3.2488654e-09 1.1299241e-15 1.0000000e+00 5.4546273e-08]\n",
      " [8.1576204e-09 6.7346388e-16 1.0000000e+00 2.1231500e-08]\n",
      " [4.0178705e-09 1.0710237e-15 1.0000000e+00 4.5767720e-08]\n",
      " [4.0133288e-09 1.0020468e-15 1.0000000e+00 4.3624741e-08]\n",
      " [2.8052942e-09 1.1997001e-15 9.9999988e-01 6.3734568e-08]\n",
      " [2.4682896e-09 1.4465514e-15 9.9999988e-01 8.1635385e-08]\n",
      " [5.4364686e-09 8.5498320e-16 1.0000000e+00 3.1969368e-08]\n",
      " [1.8718289e-09 1.9389294e-15 9.9999988e-01 1.2549620e-07]]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, verbose=1)\n",
    "\n",
    "print(y_pred[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test 길이: 2160\n",
      "y_pred 길이: 2160\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d31b33837f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 혼동 행렬 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mconf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"혼동 행렬:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_test)  # 클래스 인덱스 추출\n",
    "conf_matrix = np.zeros((len(classes), len(classes)), dtype=int)  # 혼동 행렬 초기화\n",
    "\n",
    "# 길이 확인\n",
    "print(\"y_test 길이:\", len(y_test))\n",
    "print(\"y_pred 길이:\", len(y_pred))\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "print(y_pred[0])\n",
    "y_test = y_test.astype(int)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "for i in range(len(y_test)):\n",
    "    conf_matrix[y_test[i], y_pred[i]] += 1\n",
    "\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n",
      "혼동 행렬:\n",
      " [[1080    0]\n",
      " [   0 1080]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_pred의 형태 확인 및 정수형으로 변환\n",
    "if y_pred.ndim == 2:\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "elif y_pred.ndim == 1:\n",
    "    print(\"y_pred는 이미 클래스 인덱스입니다.\")\n",
    "\n",
    "# y_test 및 y_pred의 고유 클래스 추출\n",
    "unique_classes = np.unique(np.concatenate((y_test, y_pred)))  # 두 배열의 고유 클래스 합치기\n",
    "\n",
    "# 클래스 인덱스를 0부터 시작하도록 조정\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "y_test_mapped = np.array([class_to_index[cls] for cls in y_test])\n",
    "y_pred_mapped = np.array([class_to_index[cls] for cls in y_pred])\n",
    "\n",
    "\n",
    "print(len(y_test_mapped))\n",
    "print(len(y_pred_mapped))\n",
    "# 혼동 행렬 초기화\n",
    "num_classes = len(unique_classes)\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "for i in range(len(y_test)):\n",
    "    conf_matrix[y_test_mapped[i], y_pred_mapped[i]] += 1\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
