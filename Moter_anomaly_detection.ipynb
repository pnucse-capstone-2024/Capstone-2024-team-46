{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moter dataset Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect anomalies, we plan to input data into the model at 1-second intervals with a sampling rate of 100Hz. Therefore, we will segment the data into 70 samples for each 1-second intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_0 = pd.read_csv(\"data/label_0_data.csv\")\n",
    "label_1 = pd.read_csv(\"data/label_1b_data.csv\")\n",
    "label_2 = pd.read_csv(\"data/label_2_data.csv\")\n",
    "label_3 = pd.read_csv(\"data/label_3_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359900\n",
      "359800\n",
      "359900\n",
      "359900\n"
     ]
    }
   ],
   "source": [
    "len_0 = len(label_0)\n",
    "len_1 = len(label_1)\n",
    "len_2 = len(label_2)\n",
    "len_3 = len(label_3)\n",
    "\n",
    "label_X = [label_0, label_1, label_2, label_3]\n",
    "rem = [len_0, len_1, len_2, len_3]\n",
    "\n",
    "for i in range(4):\n",
    "    r = rem[i] % 100\n",
    "    label_X[i] = label_X[i].iloc[:rem[i] - r]\n",
    "\n",
    "    print(len(label_X[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "data_split_0 = np.array_split(label_X[0], len(label_X[0]) // 100)\n",
    "data_split_1 = np.array_split(label_X[1], len(label_X[1]) // 100)\n",
    "data_split_2 = np.array_split(label_X[2], len(label_X[2]) // 100)\n",
    "data_split_3 = np.array_split(label_X[3], len(label_X[3]) // 100)\n",
    "\n",
    "\n",
    "y_label_0 = np.zeros(len(data_split_0))\n",
    "y_label_1 = np.ones(len(data_split_1))\n",
    "y_label_2 = np.full(len(data_split_2), 2)\n",
    "y_label_3 = np.full(len(data_split_3), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, validataion, test set\n",
    "\n",
    "train_ratio = 0.7\n",
    "validation ratio = 0.15\n",
    "test ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599\n",
      "3598\n",
      "3599\n",
      "3599\n",
      "x_val size: 2160\n",
      "x_test size: 2160\n",
      "y_val size: 2160\n",
      "y_test size: 2160\n",
      "14395\n",
      "14395\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "temp_ratio = 0.3\n",
    "val_ratio = 0.5\n",
    "test_ratio = temp_ratio / 2\n",
    "\n",
    "data_splits = [data_split_0, data_split_1, data_split_2, data_split_3]\n",
    "y_data_splits = [y_label_0, y_label_1, y_label_2, y_label_3]\n",
    "\n",
    "train_sizes = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "y_train_data = []\n",
    "y_test_data = []\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    data_len = len(data_splits[i])\n",
    "    train_size = int(data_len * train_ratio)\n",
    "    train_sizes.append(train_size)\n",
    "    train_data.append(data_splits[i][:train_size])\n",
    "    test_data.append(data_splits[i][train_size:])\n",
    "\n",
    "    y_train_data.append(y_data_splits[i][:train_size])\n",
    "    y_test_data.append(y_data_splits[i][train_size:])\n",
    "\n",
    "\n",
    "# check\n",
    "for i in range(4):\n",
    "    print(len(train_data[i]) + len(test_data[i]))\n",
    "\n",
    "x_train_concat = np.concatenate(train_data, axis = 0)\n",
    "x_test_concat = np.concatenate(test_data, axis = 0)\n",
    "y_train_concat = np.concatenate(y_train_data, axis=0)\n",
    "y_test_concat = np.concatenate(y_test_data, axis=0)\n",
    "\n",
    "\n",
    "val_size = int(x_test_concat.shape[0] * val_ratio)\n",
    "test_size = x_test_concat.shape[0] - val_size\n",
    "\n",
    "# validation & test set split\n",
    "x_val = x_test_concat[:val_size]\n",
    "x_test = x_test_concat[val_size:]\n",
    "\n",
    "y_val = y_test_concat[:val_size]\n",
    "y_test = y_test_concat[val_size:]\n",
    "\n",
    "# check\n",
    "print(f\"x_val size: {x_val.shape[0]}\")\n",
    "print(f\"x_test size: {x_test.shape[0]}\")\n",
    "print(f\"y_val size: {y_val.shape[0]}\")\n",
    "print(f\"y_test size: {y_test.shape[0]}\")\n",
    "\n",
    "# check \n",
    "print(x_train_concat.shape[0] + x_test.shape[0] + x_val.shape[0])\n",
    "print(y_train_concat.shape[0] + y_test.shape[0] + y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split 0: train + val + test = 2519 + 540 + 540\n",
      "Data split 1: train + val + test = 2518 + 540 + 540\n",
      "Data split 2: train + val + test = 2519 + 540 + 540\n",
      "Data split 3: train + val + test = 2519 + 540 + 540\n",
      "Train data shape: (10075, 100, 3)\n",
      "Validation data shape: (2160, 100, 3)\n",
      "Test data shape: (2160, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "temp_ratio = 0.3  # for both val and test combined\n",
    "val_ratio = 0.5  # 50% of temp_ratio for validation\n",
    "test_ratio = temp_ratio / 2  # same as val_ratio\n",
    "\n",
    "data_splits = [data_split_0, data_split_1, data_split_2, data_split_3]\n",
    "y_data_splits = [y_label_0, y_label_1, y_label_2, y_label_3]\n",
    "\n",
    "train_sizes = []\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "y_train_data = []\n",
    "y_val_data = []\n",
    "y_test_data = []\n",
    "\n",
    "for i in range(4):\n",
    "    data_len = len(data_splits[i])\n",
    "    train_size = int(data_len * train_ratio)\n",
    "    temp_size = data_len - train_size \n",
    "    val_size = int(temp_size * val_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_sizes.append(train_size)\n",
    "    train_data.append(data_splits[i][:train_size])\n",
    "    val_data.append(data_splits[i][train_size:train_size + val_size])\n",
    "    test_data.append(data_splits[i][train_size + val_size:])\n",
    "\n",
    "    # Split the labels\n",
    "    y_train_data.append(y_data_splits[i][:train_size])\n",
    "    y_val_data.append(y_data_splits[i][train_size:train_size + val_size])\n",
    "    y_test_data.append(y_data_splits[i][train_size + val_size:])\n",
    "\n",
    "# check the lengths of each split\n",
    "for i in range(4):\n",
    "    print(f\"Data split {i}: train + val + test =\", len(train_data[i]), \"+\", len(val_data[i]), \"+\", len(test_data[i]))\n",
    "\n",
    "# Concatenate all splits across the 4 data parts\n",
    "x_train_concat = np.concatenate(train_data, axis=0)\n",
    "x_val_concat = np.concatenate(val_data, axis=0)\n",
    "x_test_concat = np.concatenate(test_data, axis=0)\n",
    "\n",
    "y_train_concat = np.concatenate(y_train_data, axis=0)\n",
    "y_val_concat = np.concatenate(y_val_data, axis=0)\n",
    "y_test_concat = np.concatenate(y_test_data, axis=0)\n",
    "\n",
    "# Output final shapes to confirm correct splitting\n",
    "print(\"Train data shape:\", x_train_concat.shape)\n",
    "print(\"Validation data shape:\", x_val_concat.shape)\n",
    "print(\"Test data shape:\", x_test_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0.0: count = 540, ratio = 25.00%\n",
      "Label 1.0: count = 540, ratio = 25.00%\n",
      "Label 2.0: count = 540, ratio = 25.00%\n",
      "Label 3.0: count = 540, ratio = 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each label in y_val_concat\n",
    "unique_labels, label_counts = np.unique(y_val_concat, return_counts=True)\n",
    "\n",
    "# Calculate the percentage of each label\n",
    "label_ratios = label_counts / len(y_val_concat)\n",
    "\n",
    "# Print the results\n",
    "for label, count, ratio in zip(unique_labels, label_counts, label_ratios):\n",
    "    print(f\"Label {label}: count = {count}, ratio = {ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10075, 100, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10075,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10075, 4)\n",
      "(2160, 4)\n",
      "(2160, 4)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert y_train and y_valid to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_concat, num_classes=4)  # Assuming classes are 0, 1, 2, 3\n",
    "y_valid_one_hot = to_categorical(y_val_concat, num_classes=4)  # Assuming you have a similar y_valid\n",
    "y_test_one_hot = to_categorical(y_test_concat, num_classes=4)  # Assuming you have a similar y_test\n",
    "\n",
    "# Check the shape\n",
    "print(y_train_one_hot.shape)  # Should be (10070, 4)\n",
    "print(y_valid_one_hot.shape) \n",
    "print(y_test_one_hot.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: count = 2519, ratio = 25.00%\n",
      "Class 1: count = 2518, ratio = 24.99%\n",
      "Class 2: count = 2519, ratio = 25.00%\n",
      "Class 3: count = 2519, ratio = 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Sum along axis 0 to get the count of each class\n",
    "class_counts = np.sum(y_train_one_hot, axis=0)\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_ratios = class_counts / np.sum(class_counts)\n",
    "\n",
    "# Print the results\n",
    "for i, (count, ratio) in enumerate(zip(class_counts, class_ratios)):\n",
    "    print(f\"Class {i}: count = {int(count)}, ratio = {ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15294558232718816693\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11378904359985974365\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6933955910558672802\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15573965249812017374\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13699355476082648183\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(tf.__version__)\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import LSTM, Dropout, Dense, InputLayer\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.4735 - loss: 1.0265 - val_accuracy: 0.9991 - val_loss: 0.0031\n",
      "Epoch 2/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8578e-04 - val_accuracy: 1.0000 - val_loss: 1.6957e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.7713e-05 - val_accuracy: 0.9995 - val_loss: 3.8967e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.0000e-05 - val_accuracy: 0.9995 - val_loss: 4.6753e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0970e-05 - val_accuracy: 0.9995 - val_loss: 4.2347e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.3498e-06 - val_accuracy: 0.9995 - val_loss: 3.8241e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.0721e-06 - val_accuracy: 0.9995 - val_loss: 4.2253e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6943e-06 - val_accuracy: 0.9995 - val_loss: 4.1948e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.5990e-06 - val_accuracy: 0.9995 - val_loss: 3.6169e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.8219e-06 - val_accuracy: 0.9995 - val_loss: 3.5824e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.5435e-06 - val_accuracy: 0.9995 - val_loss: 3.8973e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2298e-06 - val_accuracy: 0.9995 - val_loss: 3.8675e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.6731e-07 - val_accuracy: 0.9995 - val_loss: 3.2931e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.7322e-07 - val_accuracy: 0.9995 - val_loss: 3.4585e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.6605e-07 - val_accuracy: 0.9995 - val_loss: 3.6332e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.5094e-07 - val_accuracy: 1.0000 - val_loss: 3.1365e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.4739e-07 - val_accuracy: 0.9995 - val_loss: 3.4021e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.0660e-07 - val_accuracy: 1.0000 - val_loss: 2.9257e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.5584e-07 - val_accuracy: 0.9995 - val_loss: 3.2632e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.8878e-07 - val_accuracy: 0.9995 - val_loss: 3.2974e-04\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # Conv Block 1 \n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\",\n",
    "                        input_shape=[100, 3]),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"), \n",
    "\n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Conv Block 2 \n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),           \n",
    "    \n",
    "    # Pooling layer\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 3\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),   \n",
    "    \n",
    "    # Pooling layer        \n",
    "    keras.layers.MaxPooling1D(pool_size=2),  \n",
    "\n",
    "    # Conv Block 4\n",
    "    keras.layers.Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # Updated output layer for multi-class classification\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")  # Change to 4 units and softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",  # Use categorical crossentropy\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])  # Accuracy metric is still appropriate\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_concat, y_train_one_hot,  # Ensure y_train is one-hot encoded for multi-class\n",
    "    epochs=20,  \n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_concat, y_valid_one_hot),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x_val_concat)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# # Save the model\n",
    "model.save(\"conv_net_v1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.9157e-07\n",
      "4.4216082528691913e-07\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test_concat, y_test_one_hot, verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "[[1.00000000e+00 9.60009633e-28 3.77273608e-08 1.64999848e-15]\n",
      " [1.00000000e+00 1.34792505e-27 5.39531690e-08 2.26325255e-15]\n",
      " [1.00000000e+00 3.60395750e-29 2.06518336e-09 1.08769433e-16]\n",
      " [1.00000000e+00 6.05255383e-29 2.87537216e-09 1.57174428e-16]\n",
      " [1.00000000e+00 5.61146893e-28 2.13649454e-08 1.01290370e-15]\n",
      " [1.00000000e+00 3.64399427e-28 1.68200565e-08 7.59562237e-16]\n",
      " [1.00000000e+00 3.92717723e-28 1.61809055e-08 7.67912023e-16]\n",
      " [1.00000000e+00 1.03308351e-28 4.70101469e-09 2.46397226e-16]\n",
      " [9.99999881e-01 1.57850104e-27 7.52927320e-08 2.81183175e-15]\n",
      " [1.00000000e+00 2.68370634e-28 1.22352048e-08 5.72222372e-16]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_concat, verbose=1)\n",
    "\n",
    "print(y_pred[:10])\n",
    "print(y_test_concat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n",
      "혼동 행렬:\n",
      " [[  0   0   0   0]\n",
      " [  0   0   0   0]\n",
      " [540 540   0   0]\n",
      " [  0   0 540 540]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_pred의 형태 확인 및 정수형으로 변환\n",
    "if y_pred.ndim == 2:\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "elif y_pred.ndim == 1:\n",
    "    print(\"y_pred는 이미 클래스 인덱스입니다.\")\n",
    "\n",
    "# y_test 및 y_pred의 고유 클래스 추출\n",
    "unique_classes = np.unique(np.concatenate((y_test, y_pred)))  # 두 배열의 고유 클래스 합치기\n",
    "\n",
    "# 클래스 인덱스를 0부터 시작하도록 조정\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "y_test_mapped = np.array([class_to_index[cls] for cls in y_test])\n",
    "y_pred_mapped = np.array([class_to_index[cls] for cls in y_pred])\n",
    "\n",
    "\n",
    "print(len(y_test_mapped))\n",
    "print(len(y_pred_mapped))\n",
    "# 혼동 행렬 초기화\n",
    "num_classes = len(unique_classes)\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "for i in range(len(y_test)):\n",
    "    conf_matrix[y_test_mapped[i], y_pred_mapped[i]] += 1\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2160"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       540\n",
      "         1.0       1.00      1.00      1.00       540\n",
      "         2.0       1.00      1.00      1.00       540\n",
      "         3.0       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00      2160\n",
      "   macro avg       1.00      1.00      1.00      2160\n",
      "weighted avg       1.00      1.00      1.00      2160\n",
      "\n",
      "혼동 행렬:\n",
      "[[540   0   0   0]\n",
      " [  0 540   0   0]\n",
      " [  0   0 540   0]\n",
      " [  0   0   0 540]]\n"
     ]
    }
   ],
   "source": [
    "## v1 손대지 말것\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_concat, y_pred_argmax))\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test_concat, y_pred_argmax)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       540\n",
      "         1.0       1.00      1.00      1.00       540\n",
      "         2.0       1.00      1.00      1.00       540\n",
      "         3.0       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00      2160\n",
      "   macro avg       1.00      1.00      1.00      2160\n",
      "weighted avg       1.00      1.00      1.00      2160\n",
      "\n",
      "혼동 행렬:\n",
      "[[540   0   0   0]\n",
      " [  0 540   0   0]\n",
      " [  0   0 540   0]\n",
      " [  0   0   0 540]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## v2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_concat, y_pred_argmax))\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test_concat, y_pred_argmax)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "print(\"혼동 행렬:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 3), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  4827672496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827676192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827848320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827850432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827726944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827729936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827732576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827734864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827739088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827741376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827741552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827991552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827995776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827998064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4827995600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4828002112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4828000528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4828004048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seyeong/anaconda3/envs/anomaly_detection/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1727610586.772481 7406712 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1727610586.773577 7406712 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-29 20:49:46.777473: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb\n",
      "2024-09-29 20:49:46.778105: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-29 20:49:46.778110: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb\n",
      "2024-09-29 20:49:46.788049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-09-29 20:49:46.789714: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-29 20:49:46.886193: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/lx/t6fh69qs69d9s21bvsl3_ynr0000gn/T/tmpnui8pisb\n",
      "2024-09-29 20:49:46.904125: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 126781 microseconds.\n",
      "2024-09-29 20:49:46.942307: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# .h5 모델 로드\n",
    "h5_model = tf.keras.models.load_model('conv_net_v1.h5')\n",
    "\n",
    "# .tflite 변환기 생성\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(h5_model)\n",
    "\n",
    "# 양자화 적용 (정수 양자화)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "\n",
    "# INT8 양자화를 위해 필요한 대표적인 데이터셋 함수 정의\n",
    "def representative_dataset():\n",
    "    for i in range(100):  # 임의로 100개의 샘플을 사용\n",
    "        # X_windows_balanced에서 일부 샘플을 선택\n",
    "        sample = x_train_concat[i]  # 각 sample의 shape은 (10, 3)\n",
    "        # shape을 (1, 10, 3)로 맞추기 위해 배치 차원 추가\n",
    "        yield [np.expand_dims(sample, axis=0).astype(np.float32)]\n",
    "\n",
    "# # INT8 양자화를 위해 필요한 대표적인 데이터셋 함수 정의\n",
    "# def representative_dataset():\n",
    "#     for _ in range(100):\n",
    "#         # (1, 10, 24)은 배치 크기 1, 10x24 크기의 입력 데이터를 의미합니다.\n",
    "#         yield [np.random.rand(1, 10, 24).astype(np.float32)]\n",
    "\n",
    "# # 대표 데이터셋 지정\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# 완전한 INT8 양자화를 위한 설정\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # INT8 연산만 허용\n",
    "converter.inference_input_type = tf.int8  # 입력 타입을 INT8로 설정\n",
    "converter.inference_output_type = tf.int8  # 출력 타입을 INT8로 설정\n",
    "\n",
    "\n",
    "# .tflite 모델로 변환\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# .tflite 모델 저장\n",
    "with open('conv_net_int8_v1.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
